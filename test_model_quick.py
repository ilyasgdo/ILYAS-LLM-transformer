import os
import torch
from transformers import GPT2TokenizerFast, GPT2LMHeadModel
import json
import time

def quick_model_test(model_path="output_best_perplexity"):
    """Test rapide du modÃ¨le entraÃ®nÃ©"""
    print("ğŸ§ª Test rapide du modÃ¨le")
    print("=" * 30)
    
    # VÃ©rification de l'existence du modÃ¨le
    if not os.path.exists(model_path):
        print(f"âŒ Erreur: Le modÃ¨le {model_path} n'existe pas")
        return
    
    print(f"ğŸ“‚ Chargement du modÃ¨le depuis: {model_path}")
    
    try:
        # Chargement
        start_time = time.time()
        tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
        model = GPT2LMHeadModel.from_pretrained(model_path)
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        load_time = time.time() - start_time
        print(f"âœ… ModÃ¨le chargÃ© en {load_time:.2f}s sur {device}")
        
        # Informations du modÃ¨le
        num_params = sum(p.numel() for p in model.parameters()) / 1e6
        print(f"ğŸ§® ParamÃ¨tres: {num_params:.1f}M")
        
        # Affichage des mÃ©triques d'entraÃ®nement
        metrics_file = os.path.join(model_path, "training_metrics.json")
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            print(f"ğŸ“Š PerplexitÃ© d'entraÃ®nement: {metrics.get('final_perplexity', 'N/A')}")
            print(f"ğŸ“‰ Loss finale: {metrics.get('final_loss', 'N/A')}")
        
        # Tests de gÃ©nÃ©ration
        test_prompts = [
            "Bonjour, comment",
            "La technologie",
            "Il Ã©tait une fois",
            "Dans le futur",
            "L'intelligence artificielle"
        ]
        
        print("\nğŸ¯ Tests de gÃ©nÃ©ration:")
        print("-" * 40)
        
        total_gen_time = 0
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n{i}. Prompt: '{prompt}'")
            
            # GÃ©nÃ©ration
            start_gen = time.time()
            inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=50,
                    temperature=0.8,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            gen_time = time.time() - start_gen
            total_gen_time += gen_time
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"   RÃ©sultat: {generated_text}")
            print(f"   Temps: {gen_time:.2f}s")
        
        # Statistiques finales
        avg_gen_time = total_gen_time / len(test_prompts)
        print(f"\nğŸ“ˆ Statistiques:")
        print(f"â±ï¸ Temps moyen de gÃ©nÃ©ration: {avg_gen_time:.2f}s")
        print(f"ğŸš€ Vitesse: {50/avg_gen_time:.1f} tokens/s")
        
        # Test de mÃ©moire GPU
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1e9
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"ğŸ’¾ MÃ©moire GPU: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)")
        
        print("\nâœ… Test terminÃ© avec succÃ¨s !")
        
    except Exception as e:
        print(f"âŒ Erreur pendant le test: {e}")
        import traceback
        traceback.print_exc()

def compare_models():
    """Compare les diffÃ©rents modÃ¨les disponibles"""
    print("ğŸ” Comparaison des modÃ¨les")
    print("=" * 30)
    
    model_dirs = [
        "output_best_perplexity",
        "output_optimized",
        "v1output"
    ]
    
    results = []
    
    for model_dir in model_dirs:
        if os.path.exists(model_dir):
            print(f"\nğŸ“‚ Analyse de {model_dir}:")
            
            # Lecture des mÃ©triques
            metrics_files = [
                os.path.join(model_dir, "training_metrics.json"),
                os.path.join(model_dir, "final_metrics.json")
            ]
            
            metrics = None
            for metrics_file in metrics_files:
                if os.path.exists(metrics_file):
                    with open(metrics_file, 'r') as f:
                        metrics = json.load(f)
                    break
            
            if metrics:
                perplexity = metrics.get('final_perplexity', 'N/A')
                loss = metrics.get('final_loss', 'N/A')
                params = metrics.get('model_parameters', 'N/A')
                
                print(f"  ğŸ“Š PerplexitÃ©: {perplexity}")
                print(f"  ğŸ“‰ Loss: {loss}")
                print(f"  ğŸ§® ParamÃ¨tres: {params}M")
                
                results.append({
                    'model': model_dir,
                    'perplexity': perplexity,
                    'loss': loss,
                    'parameters': params
                })
            else:
                print(f"  âš ï¸ Pas de mÃ©triques trouvÃ©es")
        else:
            print(f"  âŒ {model_dir} n'existe pas")
    
    # Sauvegarde de la comparaison
    if results:
        with open("model_comparison.json", 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nğŸ’¾ Comparaison sauvegardÃ©e dans model_comparison.json")
        
        # Affichage du meilleur modÃ¨le
        valid_results = [r for r in results if isinstance(r['perplexity'], (int, float))]
        if valid_results:
            best_model = min(valid_results, key=lambda x: x['perplexity'])
            print(f"\nğŸ† Meilleur modÃ¨le: {best_model['model']}")
            print(f"   PerplexitÃ©: {best_model['perplexity']}")

def main():
    """Menu principal"""
    print("ğŸ§ª Test et comparaison des modÃ¨les")
    print("=" * 40)
    
    while True:
        print("\nğŸ“‹ Options:")
        print("1. Test rapide du modÃ¨le principal")
        print("2. Comparaison des modÃ¨les")
        print("3. Test d'un modÃ¨le spÃ©cifique")
        print("4. Quitter")
        
        choice = input("\nğŸ¯ Votre choix (1-4): ").strip()
        
        if choice == '1':
            quick_model_test()
        elif choice == '2':
            compare_models()
        elif choice == '3':
            model_path = input("ğŸ“‚ Chemin du modÃ¨le: ").strip()
            if model_path:
                quick_model_test(model_path)
            else:
                print("âŒ Chemin invalide")
        elif choice == '4':
            print("ğŸ‘‹ Au revoir !")
            break
        else:
            print("âŒ Choix invalide")

if __name__ == "__main__":
    main()