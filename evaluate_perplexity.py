import os
import torch
import math
import json
from datasets import load_dataset
from transformers import (
    GPT2TokenizerFast,
    GPT2LMHeadModel,
    DataCollatorForLanguageModeling
)
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm

def calculate_perplexity(model, dataloader, device):
    """
    Calcule la perplexit√© d'un mod√®le sur un dataset
    """
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Calcul de la perplexit√©"):
            # D√©placer les donn√©es sur le device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = input_ids.clone()
            
            # Forward pass
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            
            # Calculer le nombre de tokens valides
            valid_tokens = attention_mask.sum().item()
            
            total_loss += loss.item() * valid_tokens
            total_tokens += valid_tokens
    
    # Calculer la perplexit√© moyenne
    avg_loss = total_loss / total_tokens
    perplexity = math.exp(avg_loss)
    
    return perplexity, avg_loss

def evaluate_model_detailed(model_path, test_file, batch_size=8):
    """
    √âvaluation d√©taill√©e d'un mod√®le avec m√©triques avanc√©es
    """
    print(f"üîç √âvaluation du mod√®le: {model_path}")
    print("=" * 60)
    
    # V√©rification du device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üñ•Ô∏è Device utilis√©: {device}")
    
    # Chargement du mod√®le et tokenizer
    print("üìÇ Chargement du mod√®le...")
    try:
        tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
        model = GPT2LMHeadModel.from_pretrained(model_path)
        model.to(device)
        print(f"‚úÖ Mod√®le charg√© avec succ√®s")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        return None
    
    # Chargement des donn√©es de test
    print("üìä Chargement des donn√©es de test...")
    if not os.path.exists(test_file):
        print(f"‚ùå Fichier de test non trouv√©: {test_file}")
        return None
    
    test_dataset = load_dataset('text', data_files={'test': test_file})['test']
    print(f"üìà {len(test_dataset)} exemples de test")
    
    # Tokenisation
    def tokenize_function(examples):
        return tokenizer(
            examples['text'],
            truncation=True,
            max_length=1024,
            padding=True,
            return_tensors='pt'
        )
    
    tokenized_dataset = test_dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=['text']
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # DataLoader
    dataloader = DataLoader(
        tokenized_dataset,
        batch_size=batch_size,
        collate_fn=data_collator,
        shuffle=False
    )
    
    # Calcul de la perplexit√©
    print("üßÆ Calcul de la perplexit√©...")
    perplexity, avg_loss = calculate_perplexity(model, dataloader, device)
    
    # M√©triques du mod√®le
    model_size = sum(p.numel() for p in model.parameters())
    model_size_mb = model_size * 4 / (1024 * 1024)  # Approximation en MB
    
    # R√©sultats
    results = {
        'model_path': model_path,
        'perplexity': perplexity,
        'average_loss': avg_loss,
        'model_parameters': model_size,
        'model_size_mb': model_size_mb,
        'test_samples': len(test_dataset),
        'device': str(device)
    }
    
    # Affichage des r√©sultats
    print("\nüìä R√âSULTATS D'√âVALUATION")
    print("=" * 40)
    print(f"üéØ Perplexit√©: {perplexity:.2f}")
    print(f"üìâ Loss moyenne: {avg_loss:.4f}")
    print(f"üßÆ Param√®tres: {model_size/1e6:.1f}M")
    print(f"üíæ Taille mod√®le: {model_size_mb:.1f} MB")
    print(f"üìà √âchantillons test: {len(test_dataset)}")
    
    # Interpr√©tation de la perplexit√©
    print("\nüé≠ INTERPR√âTATION")
    print("=" * 40)
    if perplexity < 20:
        print("üåü Excellent! Perplexit√© tr√®s faible")
    elif perplexity < 50:
        print("‚úÖ Bon mod√®le, perplexit√© acceptable")
    elif perplexity < 100:
        print("‚ö†Ô∏è Mod√®le moyen, peut √™tre am√©lior√©")
    else:
        print("‚ùå Perplexit√© √©lev√©e, mod√®le √† retravailler")
    
    return results

def compare_models(model_paths, test_file):
    """
    Compare plusieurs mod√®les sur la m√™me base de test
    """
    print("üèÜ COMPARAISON DE MOD√àLES")
    print("=" * 60)
    
    all_results = []
    
    for model_path in model_paths:
        if os.path.exists(model_path):
            results = evaluate_model_detailed(model_path, test_file)
            if results:
                all_results.append(results)
            print("\n" + "-" * 60 + "\n")
        else:
            print(f"‚ö†Ô∏è Mod√®le non trouv√©: {model_path}")
    
    if len(all_results) > 1:
        print("üèÖ CLASSEMENT PAR PERPLEXIT√â")
        print("=" * 40)
        
        # Tri par perplexit√© croissante (meilleur = plus faible)
        sorted_results = sorted(all_results, key=lambda x: x['perplexity'])
        
        for i, result in enumerate(sorted_results, 1):
            medal = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else f"{i}."
            print(f"{medal} {os.path.basename(result['model_path'])}: {result['perplexity']:.2f}")
        
        # Sauvegarde des r√©sultats
        with open('model_comparison.json', 'w') as f:
            json.dump(sorted_results, f, indent=2)
        print("\nüíæ R√©sultats sauvegard√©s dans: model_comparison.json")
    
    return all_results

def main():
    """
    Script principal d'√©valuation
    """
    print("üéØ √âVALUATEUR DE PERPLEXIT√â")
    print("=" * 60)
    
    # Fichier de test
    test_file = "data/test.txt"
    if not os.path.exists(test_file):
        # Utiliser le fichier de validation si pas de test
        test_file = "data/valid.txt"
        if not os.path.exists(test_file):
            print("‚ùå Aucun fichier de test trouv√© (test.txt ou valid.txt)")
            return
        print(f"‚ÑπÔ∏è Utilisation du fichier de validation: {test_file}")
    
    # Mod√®les √† √©valuer
    model_paths = [
        "output",  # Mod√®le original
        "output_best_perplexity",  # Mod√®le optimis√©
    ]
    
    # Ajouter les checkpoints r√©cents si disponibles
    if os.path.exists("output"):
        checkpoints = [d for d in os.listdir("output") if d.startswith("checkpoint-")]
        if checkpoints:
            # Prendre le checkpoint le plus r√©cent
            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split("-")[1]))
            model_paths.append(os.path.join("output", latest_checkpoint))
    
    # Filtrer les mod√®les existants
    existing_models = [path for path in model_paths if os.path.exists(path)]
    
    if not existing_models:
        print("‚ùå Aucun mod√®le trouv√© √† √©valuer")
        print("üí° Assurez-vous d'avoir entra√Æn√© un mod√®le d'abord")
        return
    
    print(f"üìã Mod√®les √† √©valuer: {len(existing_models)}")
    for model in existing_models:
        print(f"  - {model}")
    
    # √âvaluation
    if len(existing_models) == 1:
        evaluate_model_detailed(existing_models[0], test_file)
    else:
        compare_models(existing_models, test_file)

if __name__ == "__main__":
    main()