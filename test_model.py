#!/usr/bin/env python3
"""
Script de test pour le modÃ¨le LLM entraÃ®nÃ©
Permet de tester la gÃ©nÃ©ration de texte et d'Ã©valuer la qualitÃ©
"""

import os
import torch
import json
from transformers import GPT2LMHeadModel, GPT2TokenizerFast
import time

def load_model(model_path="output_optimized"):
    """Charge le modÃ¨le et le tokenizer"""
    try:
        print(f"ğŸ”„ Chargement du modÃ¨le depuis {model_path}...")
        
        # VÃ©rification de l'existence du modÃ¨le
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ModÃ¨le non trouvÃ© dans {model_path}")
        
        # Chargement
        tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
        model = GPT2LMHeadModel.from_pretrained(model_path)
        
        # Configuration GPU si disponible
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model.to(device)
        model.eval()
        
        print(f"âœ… ModÃ¨le chargÃ© sur {device}")
        print(f"ğŸ§  ParamÃ¨tres: {model.num_parameters():,}")
        
        return model, tokenizer, device
        
    except Exception as e:
        print(f"âŒ Erreur lors du chargement: {e}")
        return None, None, None

def generate_text(model, tokenizer, device, prompt, max_length=200, temperature=0.8, top_p=0.9):
    """GÃ©nÃ¨re du texte Ã  partir d'un prompt"""
    try:
        # Tokenisation du prompt
        inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
        
        # GÃ©nÃ©ration
        start_time = time.time()
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1,
                no_repeat_ngram_size=3
            )
        
        generation_time = time.time() - start_time
        
        # DÃ©codage
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Statistiques
        tokens_generated = len(outputs[0]) - len(inputs[0])
        tokens_per_second = tokens_generated / generation_time
        
        return {
            "text": generated_text,
            "tokens_generated": tokens_generated,
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second
        }
        
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration: {e}")
        return None

def interactive_test(model, tokenizer, device):
    """Mode interactif pour tester le modÃ¨le"""
    print("\nğŸ¯ Mode interactif - Tapez 'quit' pour quitter")
    print("=" * 50)
    
    while True:
        try:
            prompt = input("\nğŸ“ Entrez votre prompt: ")
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                break
            
            if not prompt.strip():
                continue
            
            print("\nğŸ”„ GÃ©nÃ©ration en cours...")
            result = generate_text(model, tokenizer, device, prompt)
            
            if result:
                print(f"\nğŸ“– Texte gÃ©nÃ©rÃ©:")
                print("-" * 40)
                print(result['text'])
                print("-" * 40)
                print(f"âš¡ {result['tokens_generated']} tokens en {result['generation_time']:.2f}s")
                print(f"ğŸš€ Vitesse: {result['tokens_per_second']:.1f} tokens/sec")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ArrÃªt demandÃ©")
            break
        except Exception as e:
            print(f"âŒ Erreur: {e}")

def benchmark_test(model, tokenizer, device):
    """Test de performance avec prompts prÃ©dÃ©finis"""
    print("\nğŸ Test de performance")
    print("=" * 30)
    
    test_prompts = [
        "Il Ã©tait une fois",
        "La science moderne",
        "Dans un monde futuriste",
        "L'intelligence artificielle",
        "Au cÅ“ur de la forÃªt"
    ]
    
    results = []
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ§ª Test {i}/5: '{prompt}'")
        
        result = generate_text(model, tokenizer, device, prompt, max_length=150)
        
        if result:
            results.append(result)
            print(f"âœ… {result['tokens_generated']} tokens - {result['tokens_per_second']:.1f} tok/s")
        else:
            print("âŒ Ã‰chec")
    
    # Statistiques globales
    if results:
        avg_speed = sum(r['tokens_per_second'] for r in results) / len(results)
        total_tokens = sum(r['tokens_generated'] for r in results)
        total_time = sum(r['generation_time'] for r in results)
        
        print(f"\nğŸ“Š RÃ©sultats globaux:")
        print(f"   Vitesse moyenne: {avg_speed:.1f} tokens/sec")
        print(f"   Total tokens: {total_tokens}")
        print(f"   Temps total: {total_time:.2f}s")
        
        return {
            "average_speed": avg_speed,
            "total_tokens": total_tokens,
            "total_time": total_time,
            "individual_results": results
        }
    
    return None

def load_metrics(model_path="output_optimized"):
    """Charge les mÃ©triques d'entraÃ®nement si disponibles"""
    metrics_file = os.path.join(model_path, "final_metrics.json")
    
    if os.path.exists(metrics_file):
        try:
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            
            print("\nğŸ“ˆ MÃ©triques d'entraÃ®nement:")
            print(f"   PerplexitÃ© finale: {metrics.get('final_perplexity', 'N/A'):.2f}")
            print(f"   Steps total: {metrics.get('total_steps', 'N/A'):,}")
            print(f"   ParamÃ¨tres: {metrics.get('model_parameters', 'N/A'):,}")
            print(f"   GPU utilisÃ©: {metrics.get('gpu_used', 'N/A')}")
            
            return metrics
        except Exception as e:
            print(f"âš ï¸ Impossible de charger les mÃ©triques: {e}")
    
    return None

def main():
    print("ğŸ§ª Test du modÃ¨le LLM entraÃ®nÃ©")
    print("=" * 40)
    
    # Chargement du modÃ¨le
    model, tokenizer, device = load_model()
    
    if model is None:
        print("âŒ Impossible de charger le modÃ¨le")
        return
    
    # Chargement des mÃ©triques
    load_metrics()
    
    # Menu principal
    while True:
        print("\nğŸ¯ Options disponibles:")
        print("1. Test interactif")
        print("2. Benchmark de performance")
        print("3. Test rapide")
        print("4. Quitter")
        
        choice = input("\nChoisissez une option (1-4): ").strip()
        
        if choice == '1':
            interactive_test(model, tokenizer, device)
        
        elif choice == '2':
            benchmark_results = benchmark_test(model, tokenizer, device)
            if benchmark_results:
                # Sauvegarde des rÃ©sultats
                with open("benchmark_results.json", "w") as f:
                    json.dump(benchmark_results, f, indent=2)
                print("ğŸ’¾ RÃ©sultats sauvegardÃ©s dans benchmark_results.json")
        
        elif choice == '3':
            print("\nğŸš€ Test rapide...")
            result = generate_text(model, tokenizer, device, "Bonjour, je suis")
            if result:
                print(f"\nğŸ“– RÃ©sultat: {result['text']}")
                print(f"âš¡ {result['tokens_per_second']:.1f} tokens/sec")
        
        elif choice == '4':
            print("ğŸ‘‹ Au revoir!")
            break
        
        else:
            print("âŒ Option invalide")

if __name__ == "__main__":
    main()