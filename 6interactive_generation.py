import os
import torch
from transformers import GPT2TokenizerFast, GPT2LMHeadModel
import json

def load_trained_model(model_path="output_best_perplexity"):
    """Charge le mod√®le entra√Æn√©"""
    print(f"üîÑ Chargement du mod√®le depuis {model_path}...")
    
    if not os.path.exists(model_path):
        print(f"‚ùå Erreur: Le dossier {model_path} n'existe pas")
        return None, None
    
    try:
        # Chargement du tokenizer
        tokenizer = GPT2TokenizerFast.from_pretrained(model_path)
        
        # Chargement du mod√®le
        model = GPT2LMHeadModel.from_pretrained(model_path)
        
        # Configuration pour la g√©n√©ration
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        print(f"‚úÖ Mod√®le charg√© avec succ√®s sur {device}")
        
        # Affichage des m√©triques si disponibles
        metrics_file = os.path.join(model_path, "training_metrics.json")
        if os.path.exists(metrics_file):
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            print(f"üìä Perplexit√© du mod√®le: {metrics.get('final_perplexity', 'N/A')}")
            print(f"üßÆ Param√®tres: {metrics.get('model_parameters', 'N/A')}M")
        
        return model, tokenizer
    
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        return None, None

def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8, top_p=0.9, num_return_sequences=1):
    """G√©n√®re du texte √† partir d'un prompt"""
    device = model.device
    
    # Encodage du prompt
    inputs = tokenizer.encode(prompt, return_tensors="pt").to(device)
    
    # G√©n√©ration
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            num_return_sequences=num_return_sequences,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1,
            no_repeat_ngram_size=2
        )
    
    # D√©codage des r√©sultats
    generated_texts = []
    for output in outputs:
        text = tokenizer.decode(output, skip_special_tokens=True)
        generated_texts.append(text)
    
    return generated_texts

def interactive_generation():
    """Interface interactive pour la g√©n√©ration de texte"""
    print("ü§ñ G√©n√©rateur de texte interactif")
    print("=" * 50)
    
    # Chargement du mod√®le
    model, tokenizer = load_trained_model()
    if model is None or tokenizer is None:
        return
    
    print("\nüìù Instructions:")
    print("- Tapez votre prompt et appuyez sur Entr√©e")
    print("- Tapez 'quit' pour quitter")
    print("- Tapez 'config' pour modifier les param√®tres")
    print("- Tapez 'help' pour l'aide")
    
    # Param√®tres par d√©faut
    config = {
        'max_length': 150,
        'temperature': 0.8,
        'top_p': 0.9,
        'num_sequences': 1
    }
    
    while True:
        print("\n" + "-" * 50)
        prompt = input("üéØ Votre prompt: ").strip()
        
        if prompt.lower() == 'quit':
            print("üëã Au revoir !")
            break
        
        elif prompt.lower() == 'help':
            print("\nüìö Aide:")
            print("- √âcrivez un d√©but de phrase ou de paragraphe")
            print("- Le mod√®le continuera votre texte")
            print("- Exemples de prompts:")
            print("  * 'Il √©tait une fois'")
            print("  * 'La technologie moderne'")
            print("  * 'Dans un monde futuriste'")
            continue
        
        elif prompt.lower() == 'config':
            print("\n‚öôÔ∏è Configuration actuelle:")
            print(f"- Longueur max: {config['max_length']}")
            print(f"- Temp√©rature: {config['temperature']}")
            print(f"- Top-p: {config['top_p']}")
            print(f"- Nombre de s√©quences: {config['num_sequences']}")
            
            try:
                new_length = input(f"Nouvelle longueur max ({config['max_length']}): ").strip()
                if new_length:
                    config['max_length'] = int(new_length)
                
                new_temp = input(f"Nouvelle temp√©rature ({config['temperature']}): ").strip()
                if new_temp:
                    config['temperature'] = float(new_temp)
                
                new_top_p = input(f"Nouveau top-p ({config['top_p']}): ").strip()
                if new_top_p:
                    config['top_p'] = float(new_top_p)
                
                new_num = input(f"Nombre de s√©quences ({config['num_sequences']}): ").strip()
                if new_num:
                    config['num_sequences'] = int(new_num)
                
                print("‚úÖ Configuration mise √† jour")
            except ValueError:
                print("‚ùå Valeur invalide, configuration inchang√©e")
            continue
        
        elif not prompt:
            print("‚ö†Ô∏è Veuillez entrer un prompt")
            continue
        
        # G√©n√©ration
        print("\nüîÑ G√©n√©ration en cours...")
        try:
            generated_texts = generate_text(
                model, tokenizer, prompt,
                max_length=config['max_length'],
                temperature=config['temperature'],
                top_p=config['top_p'],
                num_return_sequences=config['num_sequences']
            )
            
            print("\nüìù Texte(s) g√©n√©r√©(s):")
            for i, text in enumerate(generated_texts, 1):
                print(f"\n--- S√©quence {i} ---")
                print(text)
                print(f"--- Fin s√©quence {i} ---")
        
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration: {e}")

def batch_generation():
    """G√©n√©ration en lot √† partir d'une liste de prompts"""
    print("üì¶ G√©n√©ration en lot")
    print("=" * 30)
    
    # Chargement du mod√®le
    model, tokenizer = load_trained_model()
    if model is None or tokenizer is None:
        return
    
    # Prompts pr√©d√©finis
    prompts = [
        "Il √©tait une fois",
        "La science moderne",
        "Dans un futur proche",
        "L'intelligence artificielle",
        "Au c≈ìur de la for√™t"
    ]
    
    print("\nüéØ G√©n√©ration pour les prompts pr√©d√©finis:")
    
    results = []
    for i, prompt in enumerate(prompts, 1):
        print(f"\n--- Prompt {i}: '{prompt}' ---")
        try:
            generated = generate_text(model, tokenizer, prompt, max_length=100)
            print(generated[0])
            results.append({
                'prompt': prompt,
                'generated': generated[0]
            })
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
    
    # Sauvegarde des r√©sultats
    output_file = "generated_texts.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ R√©sultats sauvegard√©s dans {output_file}")

def main():
    """Menu principal"""
    print("üöÄ Utilisation du mod√®le entra√Æn√©")
    print("=" * 40)
    
    while True:
        print("\nüìã Options disponibles:")
        print("1. G√©n√©ration interactive")
        print("2. G√©n√©ration en lot")
        print("3. Quitter")
        
        choice = input("\nüéØ Votre choix (1-3): ").strip()
        
        if choice == '1':
            interactive_generation()
        elif choice == '2':
            batch_generation()
        elif choice == '3':
            print("üëã Au revoir !")
            break
        else:
            print("‚ùå Choix invalide, veuillez choisir 1, 2 ou 3")

if __name__ == "__main__":
    main()